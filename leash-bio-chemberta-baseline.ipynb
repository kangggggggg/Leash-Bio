{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":67356,"databundleVersionId":8006601,"sourceType":"competition"},{"sourceId":7856946,"sourceType":"datasetVersion","datasetId":4608382}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Update\n|version|update|cv|lb|\n|:--|:--|--:|--:|\n|v3|-|-|0.343|\n|v4|increase N_ROWS from 90M to 180M|0.573|0.465|\n|v5|increase N_SAMPLES from 1M to 2M<br>change to a larger model (5M -> 10M)|0.622|0.486|\n|v8|Add normalization commented by [@hengck23](https://www.kaggle.com/hengck23)|0.629|0.496|","metadata":{}},{"cell_type":"markdown","source":"## Update\n|version|update|cv|lb|\n|:--|:--|--:|--:|\n|v1|BaseLine|-|0.339|\n|v3|**Model Change** : DeepChem/ChemBERTa-77M-MLM|||","metadata":{}},{"cell_type":"code","source":"!pip install rdkit\n!pip install -U /kaggle/input/lightning-2-2-1/lightning-2.2.1-py3-none-any.whl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-12T07:35:24.982326Z","iopub.execute_input":"2024-06-12T07:35:24.983007Z","iopub.status.idle":"2024-06-12T07:35:55.537134Z","shell.execute_reply.started":"2024-06-12T07:35:24.982970Z","shell.execute_reply":"2024-06-12T07:35:55.536031Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting rdkit\n  Downloading rdkit-2023.9.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rdkit) (1.26.4)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from rdkit) (9.5.0)\nDownloading rdkit-2023.9.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.9/34.9 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: rdkit\nSuccessfully installed rdkit-2023.9.6\nProcessing /kaggle/input/lightning-2-2-1/lightning-2.2.1-py3-none-any.whl\nRequirement already satisfied: PyYAML<8.0,>=5.4 in /opt/conda/lib/python3.10/site-packages (from lightning==2.2.1) (6.0.1)\nRequirement already satisfied: fsspec<2025.0,>=2022.5.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning==2.2.1) (2024.2.0)\nRequirement already satisfied: lightning-utilities<2.0,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.2.1) (0.11.2)\nRequirement already satisfied: numpy<3.0,>=1.17.2 in /opt/conda/lib/python3.10/site-packages (from lightning==2.2.1) (1.26.4)\nRequirement already satisfied: packaging<25.0,>=20.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.2.1) (21.3)\nRequirement already satisfied: torch<4.0,>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.2.1) (2.1.2)\nRequirement already satisfied: torchmetrics<3.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.2.1) (1.3.2)\nRequirement already satisfied: tqdm<6.0,>=4.57.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.2.1) (4.66.1)\nRequirement already satisfied: typing-extensions<6.0,>=4.4.0 in /opt/conda/lib/python3.10/site-packages (from lightning==2.2.1) (4.9.0)\nRequirement already satisfied: pytorch-lightning in /opt/conda/lib/python3.10/site-packages (from lightning==2.2.1) (2.2.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<2025.0,>=2022.5.0->lightning==2.2.1) (3.9.1)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from lightning-utilities<2.0,>=0.8.0->lightning==2.2.1) (69.0.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging<25.0,>=20.0->lightning==2.2.1) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning==2.2.1) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning==2.2.1) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning==2.2.1) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch<4.0,>=1.13.0->lightning==2.2.1) (3.1.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning==2.2.1) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning==2.2.1) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning==2.2.1) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning==2.2.1) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning==2.2.1) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning==2.2.1) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch<4.0,>=1.13.0->lightning==2.2.1) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<4.0,>=1.13.0->lightning==2.2.1) (1.3.0)\nRequirement already satisfied: idna>=2.0 in /opt/conda/lib/python3.10/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>=2022.5.0->lightning==2.2.1) (3.6)\nInstalling collected packages: lightning\nSuccessfully installed lightning-2.2.1\n","output_type":"stream"}]},{"cell_type":"code","source":"from pathlib import Path\n\nimport numpy as np\nimport polars as pl\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchmetrics import AveragePrecision\nimport lightning as L\nfrom lightning.pytorch.callbacks import (\n    EarlyStopping,\n    ModelCheckpoint,\n    TQDMProgressBar,\n)\nfrom transformers import AutoConfig, AutoTokenizer, AutoModel, DataCollatorWithPadding\nimport datasets\nfrom rdkit import Chem","metadata":{"execution":{"iopub.status.busy":"2024-06-12T07:35:55.539500Z","iopub.execute_input":"2024-06-12T07:35:55.540233Z","iopub.status.idle":"2024-06-12T07:36:15.038131Z","shell.execute_reply.started":"2024-06-12T07:35:55.540184Z","shell.execute_reply":"2024-06-12T07:36:15.037291Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-06-12 07:36:05.864090: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-12 07:36:05.864186: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-12 07:36:06.006004: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Hyper-Parameters","metadata":{}},{"cell_type":"code","source":"DEBUG = False\nNORMALIZE = True\nN_ROWS = 180_000_000\nassert N_ROWS is None or N_ROWS % 3 == 0\nif DEBUG:\n    N_SAMPLES = 10_000\nelse:\n    N_SAMPLES = 2_000_000\nPROTEIN_NAMES = [\"BRD4\", \"HSA\", \"sEH\"]\ndata_dir = Path(\"/kaggle/input/leash-BELKA\")\nmodel_name = \"DeepChem/ChemBERTa-77M-MLM\" # v1,2 : \"DeepChem/ChemBERTa-10M-MTR\"\nbatch_size = 256\ntrainer_params = {\n  \"max_epochs\": 5,\n  \"enable_progress_bar\": True,\n  \"accelerator\": \"auto\",\n  \"precision\": \"16-mixed\",\n  \"gradient_clip_val\": None,\n  \"accumulate_grad_batches\": 1,\n  \"devices\": [0],\n}","metadata":{"execution":{"iopub.status.busy":"2024-06-12T07:36:15.039246Z","iopub.execute_input":"2024-06-12T07:36:15.039749Z","iopub.status.idle":"2024-06-12T07:36:15.046007Z","shell.execute_reply.started":"2024-06-12T07:36:15.039724Z","shell.execute_reply":"2024-06-12T07:36:15.045150Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Prepare Dataset","metadata":{}},{"cell_type":"code","source":"df = pl.read_parquet(\n    Path(data_dir, \"train.parquet\"),\n    columns=[\"molecule_smiles\", \"protein_name\", \"binds\"],\n    n_rows=N_ROWS,\n)\ntest_df = pl.read_parquet(\n    Path(data_dir, \"test.parquet\"),\n    columns=[\"molecule_smiles\"],\n    n_rows=10000 if DEBUG else None,\n)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-12T07:36:15.048276Z","iopub.execute_input":"2024-06-12T07:36:15.048543Z","iopub.status.idle":"2024-06-12T07:36:34.321734Z","shell.execute_reply.started":"2024-06-12T07:36:15.048511Z","shell.execute_reply":"2024-06-12T07:36:34.319668Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"shape: (5, 3)\n┌───────────────────────────────────┬──────────────┬───────┐\n│ molecule_smiles                   ┆ protein_name ┆ binds │\n│ ---                               ┆ ---          ┆ ---   │\n│ str                               ┆ str          ┆ i64   │\n╞═══════════════════════════════════╪══════════════╪═══════╡\n│ C#CCOc1ccc(CNc2nc(NCC3CCCN3c3ccc… ┆ BRD4         ┆ 0     │\n│ C#CCOc1ccc(CNc2nc(NCC3CCCN3c3ccc… ┆ HSA          ┆ 0     │\n│ C#CCOc1ccc(CNc2nc(NCC3CCCN3c3ccc… ┆ sEH          ┆ 0     │\n│ C#CCOc1ccc(CNc2nc(NCc3cccc(Br)n3… ┆ BRD4         ┆ 0     │\n│ C#CCOc1ccc(CNc2nc(NCc3cccc(Br)n3… ┆ HSA          ┆ 0     │\n└───────────────────────────────────┴──────────────┴───────┘","text/html":"<div><style>\n.dataframe > thead > tr,\n.dataframe > tbody > tr {\n  text-align: right;\n  white-space: pre-wrap;\n}\n</style>\n<small>shape: (5, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>molecule_smiles</th><th>protein_name</th><th>binds</th></tr><tr><td>str</td><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;C#CCOc1ccc(CNc…</td><td>&quot;BRD4&quot;</td><td>0</td></tr><tr><td>&quot;C#CCOc1ccc(CNc…</td><td>&quot;HSA&quot;</td><td>0</td></tr><tr><td>&quot;C#CCOc1ccc(CNc…</td><td>&quot;sEH&quot;</td><td>0</td></tr><tr><td>&quot;C#CCOc1ccc(CNc…</td><td>&quot;BRD4&quot;</td><td>0</td></tr><tr><td>&quot;C#CCOc1ccc(CNc…</td><td>&quot;HSA&quot;</td><td>0</td></tr></tbody></table></div>"},"metadata":{}}]},{"cell_type":"code","source":"dfs = []\nfor i, protein_name in enumerate(PROTEIN_NAMES):\n    sub_df = df[i::3]\n    sub_df = sub_df.rename({\"binds\": protein_name})\n    if i == 0:\n        dfs.append(sub_df.drop([\"id\", \"protein_name\"]))\n    else:\n        dfs.append(sub_df[[protein_name]])\ndf = pl.concat(dfs, how=\"horizontal\")\ndf = df.sample(n=N_SAMPLES)\nprint(df.head())\nprint(df[PROTEIN_NAMES].sum())","metadata":{"execution":{"iopub.status.busy":"2024-06-12T07:36:34.324814Z","iopub.execute_input":"2024-06-12T07:36:34.325626Z","iopub.status.idle":"2024-06-12T07:36:39.449666Z","shell.execute_reply.started":"2024-06-12T07:36:34.325547Z","shell.execute_reply":"2024-06-12T07:36:39.448744Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"shape: (5, 4)\n┌───────────────────────────────────┬──────┬─────┬─────┐\n│ molecule_smiles                   ┆ BRD4 ┆ HSA ┆ sEH │\n│ ---                               ┆ ---  ┆ --- ┆ --- │\n│ str                               ┆ i64  ┆ i64 ┆ i64 │\n╞═══════════════════════════════════╪══════╪═════╪═════╡\n│ Cc1ccc(C(=O)N[Dy])cc1Nc1nc(NCC2(… ┆ 0    ┆ 0   ┆ 0   │\n│ C=CC[C@@H](Nc1nc(NCc2c(F)cccc2OC… ┆ 0    ┆ 0   ┆ 0   │\n│ Cc1n[nH]c(Nc2nc(NCc3sc(N(C)C)nc3… ┆ 0    ┆ 0   ┆ 0   │\n│ COC(=O)c1cc(Nc2nc(Nc3ccc(N4CCOCC… ┆ 0    ┆ 0   ┆ 0   │\n│ C=CCOC(C)CNc1nc(NCCNC(=O)c2cccnc… ┆ 0    ┆ 0   ┆ 0   │\n└───────────────────────────────────┴──────┴─────┴─────┘\nshape: (1, 3)\n┌──────┬──────┬───────┐\n│ BRD4 ┆ HSA  ┆ sEH   │\n│ ---  ┆ ---  ┆ ---   │\n│ i64  ┆ i64  ┆ i64   │\n╞══════╪══════╪═══════╡\n│ 9831 ┆ 8406 ┆ 20040 │\n└──────┴──────┴───────┘\n","output_type":"stream"}]},{"cell_type":"code","source":"def normalize(x):\n    mol = Chem.MolFromSmiles(x)\n    smiles = Chem.MolToSmiles(mol, canonical=True, isomericSmiles=False)\n    return smiles\n\n\nif NORMALIZE:\n    df = df.with_columns(pl.col(\"molecule_smiles\").map_elements(normalize, return_dtype=pl.Utf8))\n    test_df = test_df.with_columns(pl.col(\"molecule_smiles\").map_elements(normalize, return_dtype=pl.Utf8))","metadata":{"execution":{"iopub.status.busy":"2024-06-12T07:36:39.451793Z","iopub.execute_input":"2024-06-12T07:36:39.452518Z","iopub.status.idle":"2024-06-12T08:12:30.968403Z","shell.execute_reply.started":"2024-06-12T07:36:39.452486Z","shell.execute_reply":"2024-06-12T08:12:30.967539Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_idx, val_idx = train_test_split(np.arange(len(df)), test_size=0.2)\ntrain_df, val_df = df[train_idx], df[val_idx]\nlen(train_df), len(val_df)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T08:12:30.969620Z","iopub.execute_input":"2024-06-12T08:12:30.969925Z","iopub.status.idle":"2024-06-12T08:12:31.203028Z","shell.execute_reply.started":"2024-06-12T08:12:30.969898Z","shell.execute_reply":"2024-06-12T08:12:31.202095Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(1600000, 400000)"},"metadata":{}}]},{"cell_type":"markdown","source":"# Build Dataset","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T08:12:31.204265Z","iopub.execute_input":"2024-06-12T08:12:31.204555Z","iopub.status.idle":"2024-06-12T08:12:32.941551Z","shell.execute_reply.started":"2024-06-12T08:12:31.204530Z","shell.execute_reply":"2024-06-12T08:12:32.940516Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72d92b985b514747b372df30e6c80fe0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/631 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"840646bbd350480c9eb8af65c6542bd0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/6.96k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5156e1d1383641fd849b842538245f79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/52.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4141e0c20d114ded95d2a0ae43686c2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/8.26k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27bb1c4f6473490fae8e10941c730cc7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6eb4413d38a040789c5e7e7b68cbb82e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/420 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd3f72d570ef41319df18c4e922d1872"}},"metadata":{}}]},{"cell_type":"code","source":"def tokenize(batch, tokenizer):\n    output = tokenizer(batch[\"molecule_smiles\"], truncation=True)\n    return output\n\n\nclass LMDataset(Dataset):\n    def __init__(self, df, tokenizer, stage=\"train\"):\n        assert stage in [\"train\", \"val\", \"test\"]\n        self.tokenizer = tokenizer\n        self.stage = stage\n        df = (\n            datasets.Dataset\n            .from_pandas(df.to_pandas())\n            .map(tokenize, batched=True, fn_kwargs={\"tokenizer\": self.tokenizer})\n            .to_pandas()\n        )\n        self.df = pl.from_pandas(df)\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        data = self._generate_data(index)\n        data[\"label\"] = self._generate_label(index)\n        return data        \n\n    def _generate_data(self, index):\n        data = {\n            \"input_ids\": np.array(self.df[index, \"input_ids\"]),\n            \"attention_mask\": np.array(self.df[index, \"attention_mask\"]),\n        }\n        return data\n    \n    def _generate_label(self, index):\n        if self.stage == \"test\":\n            return np.array([0, 0, 0])\n        else:\n            return self.df[index, PROTEIN_NAMES].to_numpy()[0]\n\n\nLMDataset(train_df[:100], tokenizer)[0]","metadata":{"execution":{"iopub.status.busy":"2024-06-12T08:12:32.942751Z","iopub.execute_input":"2024-06-12T08:12:32.943070Z","iopub.status.idle":"2024-06-12T08:12:33.163519Z","shell.execute_reply.started":"2024-06-12T08:12:32.943043Z","shell.execute_reply":"2024-06-12T08:12:33.162688Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"496ef5e3a7c34c609f243a2b04e9dc2e"}},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"{'input_ids': array([12, 16, 15, 20, 15, 15, 17, 16, 17, 22, 19, 18, 23, 18, 15, 15, 15,\n        20, 23, 15, 20, 25, 15, 17, 23, 15, 21, 15, 15, 17, 27, 18, 15, 15,\n        17, 27, 18, 15, 21, 18, 25, 15, 17, 23, 15, 21, 15, 15, 25, 15, 17,\n        16, 18, 25, 21, 18, 25, 20, 13], dtype=int32),\n 'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int8),\n 'label': array([0, 0, 0])}"},"metadata":{}}]},{"cell_type":"code","source":"class LBDataModule(L.LightningDataModule):\n    def __init__(self, train_df, val_df, test_df, tokenizer):\n        super().__init__()\n        self.train_df = train_df\n        self.val_df = val_df\n        self.test_df = test_df\n        self.tokenizer = tokenizer\n\n    def _generate_dataset(self, stage):\n        if stage == \"train\":\n            df = self.train_df\n        elif stage == \"val\":\n            df = self.val_df\n        elif stage == \"test\":\n            df = self.test_df\n        else:\n            raise NotImplementedError\n        dataset = LMDataset(df, self.tokenizer, stage=stage)\n        return dataset\n\n    def _generate_dataloader(self, stage):\n        dataset = self._generate_dataset(stage)\n        if stage == \"train\":\n            shuffle=True\n            drop_last=True\n        else:\n            shuffle=False\n            drop_last=False\n        return DataLoader(\n            dataset,\n            batch_size=batch_size,\n            shuffle=shuffle,\n            drop_last=drop_last,\n            pin_memory=True,\n            collate_fn=DataCollatorWithPadding(self.tokenizer),\n        )\n\n    def train_dataloader(self):\n        return self._generate_dataloader(\"train\")\n\n    def val_dataloader(self):\n        return self._generate_dataloader(\"val\")\n\n    def test_dataloader(self):\n        return self._generate_dataloader(\"test\")\n    \n    \ndatamodule = LBDataModule(train_df, val_df, test_df, tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T08:12:33.164778Z","iopub.execute_input":"2024-06-12T08:12:33.165157Z","iopub.status.idle":"2024-06-12T08:12:33.177341Z","shell.execute_reply.started":"2024-06-12T08:12:33.165123Z","shell.execute_reply":"2024-06-12T08:12:33.176309Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Build Model","metadata":{}},{"cell_type":"code","source":"class LMModel(nn.Module):\n    def __init__(self, model_name):\n        super().__init__()\n        self.config = AutoConfig.from_pretrained(model_name, num_labels=3)\n        self.lm = AutoModel.from_pretrained(model_name, add_pooling_layer=False)\n        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n        self.classifier = nn.Linear(self.config.hidden_size, self.config.num_labels)\n        self.loss_fn = nn.BCEWithLogitsLoss(reduction=\"mean\")\n\n    def forward(self, batch):\n        last_hidden_state = self.lm(\n            batch[\"input_ids\"],\n            attention_mask=batch[\"attention_mask\"],\n        ).last_hidden_state\n        logits = self.classifier(\n            self.dropout(last_hidden_state[:, 0])\n        )\n        return {\n            \"logits\": logits,\n        }\n\n    def calculate_loss(self, batch):\n        output = self.forward(batch)\n        loss = self.loss_fn(output[\"logits\"], batch[\"labels\"].float())\n        output[\"loss\"] = loss\n        return output\n\n    \nLMModel(model_name)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T08:12:33.181189Z","iopub.execute_input":"2024-06-12T08:12:33.181574Z","iopub.status.idle":"2024-06-12T08:12:35.319921Z","shell.execute_reply.started":"2024-06-12T08:12:33.181549Z","shell.execute_reply":"2024-06-12T08:12:35.319011Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/13.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55c5c38062cd4c7ca57ad0847727104a"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"LMModel(\n  (lm): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(600, 384, padding_idx=1)\n      (position_embeddings): Embedding(515, 384, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 384)\n      (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.144, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-2): 3 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=384, out_features=384, bias=True)\n              (key): Linear(in_features=384, out_features=384, bias=True)\n              (value): Linear(in_features=384, out_features=384, bias=True)\n              (dropout): Dropout(p=0.109, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=384, out_features=384, bias=True)\n              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.144, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=384, out_features=464, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=464, out_features=384, bias=True)\n            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.144, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.144, inplace=False)\n  (classifier): Linear(in_features=384, out_features=3, bias=True)\n  (loss_fn): BCEWithLogitsLoss()\n)"},"metadata":{}}]},{"cell_type":"code","source":"class LBModelModule(L.LightningModule):\n    def __init__(self, model_name):\n        super().__init__()\n        self.model = LMModel(model_name)\n        self.map = AveragePrecision(task=\"binary\")\n\n    def forward(self, batch):\n        return self.model(batch)\n\n    def calculate_loss(self, batch, batch_idx):\n        return self.model.calculate_loss(batch)\n\n    def training_step(self, batch, batch_idx):\n        ret = self.calculate_loss(batch, batch_idx)\n        self.log(\"train_loss\", ret[\"loss\"], on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)\n        return ret[\"loss\"]\n\n    def validation_step(self, batch, batch_idx):\n        ret = self.calculate_loss(batch, batch_idx)\n        self.log(\"val_loss\", ret[\"loss\"], on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n        self.map.update(F.sigmoid(ret[\"logits\"]), batch[\"labels\"].long())\n\n    def on_validation_epoch_end(self):\n        val_map = self.map.compute()\n        self.log(\"val_map\", val_map, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n        self.map.reset()\n\n    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n        logits = self.forward(batch)[\"logits\"]\n        probs = F.sigmoid(logits)\n        return probs\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=0.0001)\n        return {\n            \"optimizer\": optimizer,\n        }\n\n    \nmodelmodule = LBModelModule(model_name)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T08:12:35.321207Z","iopub.execute_input":"2024-06-12T08:12:35.321587Z","iopub.status.idle":"2024-06-12T08:12:35.617345Z","shell.execute_reply.started":"2024-06-12T08:12:35.321552Z","shell.execute_reply":"2024-06-12T08:12:35.616518Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"checkpoint_callback = ModelCheckpoint(\n    filename=f\"model-{{val_map:.4f}}\",\n    save_weights_only=True,\n    monitor=\"val_map\",\n    mode=\"max\",\n    dirpath=\"/kaggle/working\",\n    save_top_k=1,\n    verbose=1,\n)\nearly_stop_callback = EarlyStopping(monitor=\"val_map\", mode=\"max\", patience=3)\nprogress_bar_callback = TQDMProgressBar(refresh_rate=1)\ncallbacks = [\n    checkpoint_callback,\n    early_stop_callback,\n    progress_bar_callback,\n]","metadata":{"execution":{"iopub.status.busy":"2024-06-12T08:12:35.618562Z","iopub.execute_input":"2024-06-12T08:12:35.619008Z","iopub.status.idle":"2024-06-12T08:12:35.632243Z","shell.execute_reply.started":"2024-06-12T08:12:35.618951Z","shell.execute_reply":"2024-06-12T08:12:35.631521Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"trainer = L.Trainer(callbacks=callbacks, **trainer_params)\ntrainer.fit(modelmodule, datamodule)","metadata":{"execution":{"iopub.status.busy":"2024-06-12T08:12:35.633343Z","iopub.execute_input":"2024-06-12T08:12:35.633682Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"INFO: Using 16bit Automatic Mixed Precision (AMP)\nINFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: IPU available: False, using: 0 IPUs\nINFO: HPU available: False, using: 0 HPUs\nWARNING: Missing logger folder: /kaggle/working/lightning_logs\n/opt/conda/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /kaggle/working exists and is not empty.\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n  | Name  | Type                   | Params\n-------------------------------------------------\n0 | model | LMModel                | 3.3 M \n1 | map   | BinaryAveragePrecision | 0     \n-------------------------------------------------\n3.3 M     Trainable params\n0         Non-trainable params\n3.3 M     Total params\n13.123    Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/400000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90620cbae1574b658399318023577dfd"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1600000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a00de01c39384992ae76e40cb06d9c92"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbd8c1ded07b46eb9d285a27f087077e"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"test_df = pl.read_parquet(\n    Path(data_dir, \"test.parquet\"),\n    columns=[\"molecule_smiles\"],\n    n_rows=10000 if DEBUG else None,\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"working_dir = Path(\"/kaggle/working\")\nmodel_paths = working_dir.glob(\"*.ckpt\")\ntest_dataloader = datamodule.test_dataloader()\nfor model_path in model_paths:\n    print(model_path)\n    modelmodule = LBModelModule.load_from_checkpoint(\n        checkpoint_path=model_path,\n        model_name=model_name,\n    )\n    predictions = trainer.predict(modelmodule, test_dataloader)\n    predictions = torch.cat(predictions).numpy()\n    pred_dfs = []\n    for i, protein_name in enumerate(PROTEIN_NAMES):\n        pred_dfs.append(\n            test_df.with_columns(\n                pl.lit(protein_name).alias(\"protein_name\"),\n                pl.lit(predictions[:, i]).alias(\"binds\"),\n            )\n        )\n    pred_df = pl.concat(pred_dfs)\n    submit_df = (\n        pl.read_parquet(Path(data_dir, \"test.parquet\"), columns=[\"id\", \"molecule_smiles\", \"protein_name\"])\n        .join(pred_df, on=[\"molecule_smiles\", \"protein_name\"], how=\"left\")\n        .select([\"id\", \"binds\"])\n        .sort(\"id\")\n    )\n    submit_df.write_csv(Path(working_dir, f\"submission_{model_path.stem}.csv\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ensemble","metadata":{}},{"cell_type":"code","source":"sub_files = list(working_dir.glob(\"submission_*.csv\"))\nsub_files","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sub_dfs = []\nfor sub_file in sub_files:\n    sub_dfs.append(pl.read_csv(sub_file))\nsubmit_df = (\n    pl.concat(sub_dfs)\n    .group_by(\"id\")\n    .agg(pl.col(\"binds\").mean())\n    .sort(\"id\")\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!rm -fr *","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit_df.write_csv(Path(working_dir, \"submission.csv\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Future Directions\n- Finding the optimal CV strategy\n- Increase data\n- Utilize buildingblock1 ~ buildingblock3\n- Large-scale models\n- Tune hyper-parameters\n- Ensemble\n- etc...","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}